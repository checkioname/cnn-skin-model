modelo rodou com as configura√ßoes:

for i, obj in enumerate(tests):
    lst = len(os.listdir('runs/'))
    writer = SummaryWriter(f"runs/ml-model-test-{lst}")
    model = NeuralNetwork(tests[i], dropout_prob=0.1).to(device)
    
    momentum = 0.9
    weight_decay = 0.001
    loss_fn = nn.BCELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=momentum)
    
    # Crie um objeto StepLR para ajustar a taxa de aprendizado
    scheduler = StepLR(optimizer, step_size=20, gamma=0.2)
    #scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=10, verbose=True)

As camadas convolucionais e input e output size continuam o mesmo dos testes anteriores
