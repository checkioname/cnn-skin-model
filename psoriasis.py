# -*- coding: utf-8 -*-
"""psoriasis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gIXi45gJfR0l-C62baoo5ozgyModiltI
"""

#Conectar com o google drive
from google.colab import drive
drive.mount('/content/drive')

pip install torch_xla py7zr

#Exportar os arquivos na pasta certa
from py7zr import SevenZipFile

#/content/gdrive/MyDrive/psoriasis.7z
with SevenZipFile('/content/drive/MyDrive/psoriasis/test_out.7z', mode='r') as z:
    z.extractall(path='/content/sample_data/')

import os
from random import randint
import torch
import torchvision
import numpy as np
import pandas as pd
from torchvision.io import read_image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import StepLR
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.nn.functional as F
from torch.utils.data import Dataset
from PIL import Image
from sklearn.model_selection import StratifiedKFold
from torchvision import datasets,transforms
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader, Subset
import torch_xla.core.xla_model as xm

# # Set PYTORCH_CUDA_ALLOC_CONF environment variable
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:1024"

#Pytorch possibilida o usa facil de gpu
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else xm.xla_device()
    if xm.xla_device() is not None
    else "cpu"
)

print(f"Using {device} device")

transforms = transforms.Compose([
    transforms.Resize((150,150)),
    # transforms.RandomHorizontalFlip(p=0.5),
    # transforms.RandomVerticalFlip(p=0.5),
    transforms.ToTensor(),  # Converte para tensor
])

class CustomDataset(Dataset):
    def __init__(self, csv_file, img_dir, transform=None, target_transform=None):
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

        self.labels = [str(label) for label in self.data['label']]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        image_name = self.data.loc[idx, 'image_path']
        image_path = os.path.join(self.img_dir, image_name)
        image = Image.open(image_path)
        label = str(self.data.loc[idx, 'label'])

        if self.transform:
          image = self.transform(image)
        if self.target_transform:
          label = self.transform(label)

        return image, label

batch_size = 32
num_folds = 5

"""## **Fazendo Validação cruzada e salvando o o dataloader no drive**
(K-fold estratificada, para manter as proporções das classes)
<br>Não precisa rodar caso ja tenha executado antes
"""

# Criando KFold cross-validator
kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

# Criando DataLoader para o conjunto de treinamento
custom_dataset = CustomDataset(csv_file='/content/drive/MyDrive/psoriasis/augmented_data.csv', img_dir='/content/sample_data/', transform=transforms, target_transform=None)

labels = custom_dataset.labels
for fold, (train_index, val_index) in enumerate(kf.split(range(len(labels)), labels)):
    print(f"Fold {fold + 1}/{num_folds}")

    # Salvando os índices de treinamento e validação
    torch.save(train_index, os.path.join(f'/content/drive/MyDrive/psoriasis/train_index_fold{fold}.pt'))
    torch.save(val_index, os.path.join(f'/content/drive/MyDrive/psoriasis/val_index_fold{fold}.pt'))

    print(train_index, val_index)

loaded_val_fold_loader = torch.load('/content/drive/MyDrive/psoriasis/fold_loader_fold0.pt')

print(loaded_val_fold_loader.size())

"""## **Código com o modelo e função de treino e teste**
Somente carregar os dataloader da celula anterior
"""

def get_data_dimensions(dataloader):
    for batch, (X, y) in enumerate(dataloader):
        print(batch)
        print(f"Shape of X [N, C, H, W]: {X.shape}")
        print(f'Length of X: {len(X)}')
        print(f"Length/type of y: {len(y)} {type(y)}")
        print(f"Size of dataloader: {len(dataloader)}")
        break

# Display image and label.
def plot_data(dataloader,img_num):
    train_features, train_labels = next(iter(dataloader))
    for i in range(img_num):
        i = randint(0,len(train_features)-1)
        img = train_features[i].squeeze().permute(1, 2, 0)
        label = train_labels[i]
        plt.imshow(img, cmap="gray")
        plt.show()
        print(f"Label: {label}")



test_15 = [18, 33, 225, 129, 63, 18]
test_16 = [33, 33, 129, 129, 63, 33]
test_17 = [33, 33, 63, 63, 33, 33]



tests = [test_15]


# Defina uma semente para a inicialização
seed = 42
torch.manual_seed(seed)

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        super(DepthwiseSeparableConv, self).__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)
        self.batch_norm1 = nn.BatchNorm2d(in_channels)
        self.batch_norm2 = nn.BatchNorm2d(in_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.batch_norm1(x)
        x = self.relu(x)
        x = self.depthwise(x)
        x = self.batch_norm2(x)
        x = self.relu(x)
        x = self.pointwise(x)
        return x

class NeuralNetwork(nn.Module):
    def __init__(self, dropout_prob):
        super(NeuralNetwork, self).__init__()
        # Input image size for your specific problem
        self.input_size = (3, 224, 224)

        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            DepthwiseSeparableConv(32, 64, 1),
            DepthwiseSeparableConv(64, 128, 2),
            DepthwiseSeparableConv(128, 128, 1),
            DepthwiseSeparableConv(128, 256, 2),
            DepthwiseSeparableConv(256, 256, 1),
            DepthwiseSeparableConv(256, 512, 2),
            nn.AdaptiveAvgPool2d(1),
        )

        self.dropout = nn.Dropout(p=dropout_prob)

        flattened_size = self._get_flattened_size(self.input_size)

        self.fc_layers = nn.Sequential(
            Flatten(),
            nn.Linear(flattened_size, 256),  # Adjust the size based on your needs
            nn.ReLU(),
            nn.Dropout(p=dropout_prob),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x

    def _get_flattened_size(self, input_size):
        with torch.no_grad():
            dummy_input = torch.zeros(1, *input_size)
            output = self.conv_layers(dummy_input)
            flattened_output = output.view(output.size(0), -1)
            return flattened_output.size(1)



#batch_labels_numeric = [class_to_idx[label] for label in batch_labels]
class_to_idx = {"psoriasis": 0, "melanome": 1}


def testing_entries(model,dataloader):
    size = len(dataloader)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        batch_labels_numeric = [class_to_idx[label] for label in y]
        batch_labels_tensor = torch.tensor(batch_labels_numeric).float()
        #print(batch)
        print('shape tensor imagem:',X.shape)
        print('shape tensor y antes tranformacao:',len(y))
        print('shape tensor label:',batch_labels_tensor.shape)
        print(batch_labels_tensor)
        break



# Função de treinamento
def train(dataloader, model, loss_fn, optimizer, class_to_idx, writer, device,epoch):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        batch_labels_numeric = [class_to_idx[label] for label in y]
        y = torch.tensor(batch_labels_numeric, dtype=torch.float32).to(device)
        X, y = X.to(device), y.to(device)

        optimizer.zero_grad()

        pred = model(X)
        pred = pred.squeeze(1)

        loss = loss_fn(pred, y)
        writer.add_scalar("Loss/train", loss, epoch)
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            current = batch * len(X)
            print(f"Loss: {loss.item():.7f}  [{current:>5d}/{size:>5d}]")

# Função de teste
def test(data, model, loss_fn, class_to_idx, epoch, writer, device):
    size = len(data.dataset)
    num_batches = len(data)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in data:
            batch_labels_numeric = [class_to_idx[label] for label in y]
            y = torch.tensor(batch_labels_numeric, dtype=torch.float32).to(device)
            X, y = X.to(device), y.to(device)
            pred = model(X)
            pred = pred.squeeze(1)
            test_loss += loss_fn(pred, y).item()
            #Somando os acertos
            correct += (torch.round(pred) == y).sum().item()

    test_loss /= num_batches
    correct /= size
    accuracy = 100 * correct
    writer.add_scalar("Loss/test", test_loss, epoch)
    writer.add_scalar("Accuracy/test", accuracy, epoch)
    print(f"Test Error: Accuracy: {accuracy:.2f}% | Avg loss: {test_loss:.8f}\n")

# Exemplo de uso
class_to_idx = {"psoriasis": 1, "melanome": 0}

# Definir as funções de perda e otimizadores
e = int(input("Digite o número de épocas: "))



# Crie um objeto StepLR para ajustar a taxa de aprendizado
#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)

for i, obj in enumerate(tests):
    print('rodando camadas: ',obj)

    if os.path.exists('/content/drive/MyDrive/psoriasis/runs') == False:
      os.mkdir('/content/drive/MyDrive/psoriasis/runs')
    lst = len(os.listdir('/content/drive/MyDrive/psoriasis/runs'))

    for fold in range(num_folds):
      #Acompanhando o treinamento de com cada fold
      writer = SummaryWriter(f"/content/drive/MyDrive/psoriasis/runs/ml-model_depthpool-test-{lst}/fold{fold}")


      train_index = torch.load(f'/content/drive/MyDrive/psoriasis/train_index_fold{fold}.pt')
      val_index = torch.load(f'/content/drive/MyDrive/psoriasis/val_index_fold{fold}.pt')

      custom_dataset = CustomDataset(csv_file='/content/drive/MyDrive/psoriasis/augmented_data.csv', img_dir='/content/sample_data/', transform=transforms, target_transform=None)

      #conjunto de treino e teste
      train_loader = DataLoader(Subset(custom_dataset, train_index), batch_size=batch_size, shuffle=True)
      test_loader = DataLoader(Subset(custom_dataset, val_index), batch_size=batch_size, shuffle=True)

      #definindo a rede e hiperparametros
      model = NeuralNetwork(dropout_prob=0.1).to(device)
      momentum = 0.9
      weight_decay = 0.001
      learning_rate = 0.01


      loss_fn = nn.BCELoss()

      optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
      scheduler = StepLR(optimizer, step_size=20, gamma=0.5)



      #writer.add_graph(model, torch.randn(32, 3, 224, 224).to(device))
      # Registre os detalhes no TensorBoard
      writer.add_text("Model Configuration", str(model))
      writer.add_text("Scheduler", str(scheduler))
      writer.add_text("Optimizer Configuration", str(optimizer))
      writer.add_text("Loss Function", str(loss_fn))
      writer.add_hparams({"learning_rate": learning_rate, "batch_size": batch_size, "momentum": momentum,"weight decay": weight_decay}, {})

      print(f"Teste - {i+1}")
      for t in range(e):
          print(f"Epoch {t+1}\n-------------------------------")
          train(train_loader, model, loss_fn, optimizer, class_to_idx, writer, device, t)
          test(test_loader, model, loss_fn, class_to_idx, t, writer, device)
          scheduler.step()



      print("Done!")
      writer.flush()
      writer.close()
      torch.save({
              'epoch': t,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              }, f'/content/drive/MyDrive/psoriasis/runs/ml-model_depthpool-test-{lst}/fold{fold}/model.pt')

"""# **Calcular global avg pooling**"""

import torch
import torch.nn as nn

# Imagem totalmente preta
img_preta = torch.zeros((1, 2, 20, 20))

# Imagem totalmente branca
img_branca = torch.ones((1, 2, 20, 20))

# Função para calcular o Global Average Pooling
def calcular_global_avg_pooling(img):
    global_avg_pooling = nn.AdaptiveAvgPool2d(1)
    output_pooled = global_avg_pooling(img).view(img.size(0), -1)
    return output_pooled

# Calcular Global Average Pooling para imagens preta e branca
output_pooled_preta = calcular_global_avg_pooling(img_preta)
output_pooled_branca = calcular_global_avg_pooling(img_branca)

# Exibir os resultados
print("Resultado para a imagem preta:", output_pooled_preta)
print("Resultado para a imagem branca:", output_pooled_branca)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/psoriasis/runs --port 6061

"""# **Testando a saída das imagens pelas convoluções**"""

#testar algumas saidas da camada convolucional
import torch.nn as nn
import torch

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        super(DepthwiseSeparableConv, self).__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)
        self.batch_norm1 = nn.BatchNorm2d(in_channels)
        self.batch_norm2 = nn.BatchNorm2d(in_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.batch_norm1(x)
        x = self.relu(x)
        x = self.depthwise(x)
        x = self.batch_norm2(x)
        x = self.relu(x)
        x = self.pointwise(x)
        return x

class NeuralNetwork(nn.Module):
    def __init__(self, dropout_prob):
        super(NeuralNetwork, self).__init__()
        # Input image size for your specific problem
        self.input_size = (3, 224, 224)

        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            DepthwiseSeparableConv(32, 64, 1),
            DepthwiseSeparableConv(64, 128, 2),
            DepthwiseSeparableConv(128, 128, 1),
            DepthwiseSeparableConv(128, 256, 2),
            DepthwiseSeparableConv(256, 256, 1),
            DepthwiseSeparableConv(256, 512, 2),
            nn.AdaptiveAvgPool2d(1),
        )

        self.dropout = nn.Dropout(p=dropout_prob)

        flattened_size = self._get_flattened_size(self.input_size)

        self.fc_layers = nn.Sequential(
            Flatten(),
            nn.Linear(flattened_size, 256),  # Adjust the size based on your needs
            nn.ReLU(),
            nn.Dropout(p=dropout_prob),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x

    def _get_flattened_size(self, input_size):
        with torch.no_grad():
            dummy_input = torch.zeros(1, *input_size)
            output = self.conv_layers(dummy_input)
            flattened_output = output.view(output.size(0), -1)
            return flattened_output.size(1)




test = [18, 33, 225, 129, 63, 33]
model = NeuralNetwork(0.1)


print(model._get_flattened_size((3,224,224)))